{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries required\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import sum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import rank, col, unix_timestamp, from_unixtime, to_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "timeFmt = \"yyyy-MM-dd\"\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading MIMIC data\n",
    "df = spark.read.csv(\"mimic_master.csv\", header='true', inferSchema='true')\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52643, 16)\n"
     ]
    }
   ],
   "source": [
    "#count row and column to explore\n",
    "print((df.count(), len(df.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- paramCT: integer (nullable = true)\n",
      " |-- gpiCT: integer (nullable = true)\n",
      " |-- ndcCT: integer (nullable = true)\n",
      " |-- ahfsCT: integer (nullable = true)\n",
      " |-- medCT: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birth_date: timestamp (nullable = true)\n",
      " |-- F_T2D_Diag: timestamp (nullable = true)\n",
      " |-- F_T1D_Diag: timestamp (nullable = true)\n",
      " |-- F_LD_Diag: timestamp (nullable = true)\n",
      " |-- F_KD_Diag: timestamp (nullable = true)\n",
      " |-- F_CVD_Diag: timestamp (nullable = true)\n",
      " |-- F_ALZ_Diag: timestamp (nullable = true)\n",
      " |-- F_ALZD_Diag: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Certain selected columns as required\n",
    "df1=df.select('person_id','age','gender','birth_date','F_T2D_Diag','F_ALZ_Diag','F_ALZD_Diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+-------------------+-------------------+----------+-----------+\n",
      "|person_id|age|gender|         birth_date|         F_T2D_Diag|F_ALZ_Diag|F_ALZD_Diag|\n",
      "+---------+---+------+-------------------+-------------------+----------+-----------+\n",
      "|      148| 78|     F|2029-07-11 00:00:00|               null|      null|       null|\n",
      "|      463| 62|     F|2136-09-25 00:00:00|               null|      null|       null|\n",
      "|      471| 75|     F|2046-08-30 00:00:00|               null|      null|       null|\n",
      "|      833|  0|     M|2137-05-23 00:00:00|               null|      null|       null|\n",
      "|     1088| 68|     M|2102-03-05 00:00:00|               null|      null|       null|\n",
      "|     1238|  0|     F|2197-03-27 00:00:00|               null|      null|       null|\n",
      "|     1342| 72|     F|2034-03-20 00:00:00|               null|      null|       null|\n",
      "|     1580| 44|     F|2081-05-14 00:00:00|               null|      null|       null|\n",
      "|     1591|  0|     M|2106-04-01 00:00:00|               null|      null|       null|\n",
      "|     1645| 43|     F|2103-09-22 00:00:00|               null|      null|       null|\n",
      "|     1829| 53|     M|2133-06-23 00:00:00|2187-04-17 14:00:00|      null|       null|\n",
      "|     1959|  0|     F|2147-03-24 00:00:00|               null|      null|       null|\n",
      "|     2122| 33|     M|2124-11-23 00:00:00|               null|      null|       null|\n",
      "|     2142|  0|     M|2171-05-11 00:00:00|               null|      null|       null|\n",
      "|     2366| 36|     F|2113-08-18 00:00:00|               null|      null|       null|\n",
      "|     2659| 18|     M|2154-02-24 00:00:00|               null|      null|       null|\n",
      "|     3175| 32|     F|2105-05-30 00:00:00|               null|      null|       null|\n",
      "|     3794| 77|     F|2029-09-30 00:00:00|               null|      null|       null|\n",
      "|     3918| 62|     M|2119-03-06 00:00:00|               null|      null|       null|\n",
      "|     3997| 62|     M|2042-12-07 00:00:00|               null|      null|       null|\n",
      "+---------+---+------+-------------------+-------------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52643, 7)\n"
     ]
    }
   ],
   "source": [
    "# Number of Rows and Columns in the sub dataset\n",
    "print((df1.count(), len(df1.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birth_date: timestamp (nullable = true)\n",
      " |-- F_T2D_Diag: timestamp (nullable = true)\n",
      " |-- F_ALZ_Diag: timestamp (nullable = true)\n",
      " |-- F_ALZD_Diag: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema of the dataset\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person_id',\n",
       " 'age',\n",
       " 'gender',\n",
       " 'birth_date',\n",
       " 'F_T2D_Diag',\n",
       " 'F_ALZ_Diag',\n",
       " 'F_ALZD_Diag']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|23184|\n",
      "|     M|29459|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby([\"gender\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age diag\n",
    "Age_T2D_diag =F.round((F.col(\"F_T2D_Diag\").cast(\"long\") - F.col(\"birth_date\").cast(\"long\"))/(365*60*60*24), 3)\n",
    "Age_AD_diag =F.round((F.col(\"F_ALZ_Diag\").cast(\"long\") - F.col(\"birth_date\").cast(\"long\"))/(365*60*60*24), 3)\n",
    "Age_T2DAD_both_diag=F.round((F.col(\"F_T2D_Diag\").cast(\"long\") - F.col(\"F_ALZ_Diag\").cast(\"long\"))/(365*60*60*24), 3)\n",
    "Age_DEM_diag=F.round((F.col(\"F_ALZD_Diag\").cast(\"long\") - F.col(\"F_ALZ_Diag\").cast(\"long\"))/(365*60*60*24), 3)\n",
    "Age_T2DDEM_both_diag=F.round((F.col(\"F_T2D_Diag\").cast(\"long\") - F.col(\"F_ALZD_Diag\").cast(\"long\"))/(365*60*60*24), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.withColumn(\"T2D_ONLY\",Age_T2D_diag).withColumn(\"AD_ONLY\",Age_AD_diag).withColumn(\"BOTH_T2D_AD\"\\\n",
    ",Age_T2DAD_both_diag).withColumn(\"DEM_ONLY\",Age_DEM_diag).withColumn(\"BOTH_AD_DEM\",Age_T2DDEM_both_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+-------------------+----------+----------+-----------+--------+-------+-----------+--------+-----------+\n",
      "|person_id|age|gender|         birth_date|F_T2D_Diag|F_ALZ_Diag|F_ALZD_Diag|T2D_ONLY|AD_ONLY|BOTH_T2D_AD|DEM_ONLY|BOTH_AD_DEM|\n",
      "+---------+---+------+-------------------+----------+----------+-----------+--------+-------+-----------+--------+-----------+\n",
      "|      148| 78|     F|2029-07-11 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "|      463| 62|     F|2136-09-25 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "|      471| 75|     F|2046-08-30 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "+---------+---+------+-------------------+----------+----------+-----------+--------+-------+-----------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[person_id: int, age: int, gender: string, birth_date: timestamp, F_T2D_Diag: timestamp, F_ALZ_Diag: timestamp, F_ALZD_Diag: timestamp, T2D_ONLY: double, AD_ONLY: double, BOTH_T2D_AD: double, DEM_ONLY: double, BOTH_AD_DEM: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person_id',\n",
       " 'age',\n",
       " 'gender',\n",
       " 'birth_date',\n",
       " 'F_T2D_Diag',\n",
       " 'F_ALZ_Diag',\n",
       " 'F_ALZD_Diag',\n",
       " 'T2D_ONLY',\n",
       " 'AD_ONLY',\n",
       " 'BOTH_T2D_AD',\n",
       " 'DEM_ONLY',\n",
       " 'BOTH_AD_DEM']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.filter(df1.age >=100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1.filter(df1.age <=110) #filtering invalid dates as age with 300 or more found in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.describe().show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birth_date: timestamp (nullable = true)\n",
      " |-- F_T2D_Diag: timestamp (nullable = true)\n",
      " |-- F_ALZ_Diag: timestamp (nullable = true)\n",
      " |-- F_ALZD_Diag: timestamp (nullable = true)\n",
      " |-- T2D_ONLY: double (nullable = true)\n",
      " |-- AD_ONLY: double (nullable = true)\n",
      " |-- BOTH_T2D_AD: double (nullable = true)\n",
      " |-- DEM_ONLY: double (nullable = true)\n",
      " |-- BOTH_AD_DEM: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+-------------------+----------+----------+-----------+--------+-------+-----------+--------+-----------+\n",
      "|person_id|age|gender|         birth_date|F_T2D_Diag|F_ALZ_Diag|F_ALZD_Diag|T2D_ONLY|AD_ONLY|BOTH_T2D_AD|DEM_ONLY|BOTH_AD_DEM|\n",
      "+---------+---+------+-------------------+----------+----------+-----------+--------+-------+-----------+--------+-----------+\n",
      "|      148| 78|     F|2029-07-11 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "|      463| 62|     F|2136-09-25 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "|      471| 75|     F|2046-08-30 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "|      833|  0|     M|2137-05-23 00:00:00|      null|      null|       null|    null|   null|       null|    null|       null|\n",
      "+---------+---+------+-------------------+----------+----------+-----------+--------+-------+-----------+--------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df2.select('person_id','gender','T2D_ONLY','AD_ONLY','DEM_ONLY',\\\n",
    "                'BOTH_T2D_AD','BOTH_AD_DEM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing data checking and handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+-------+--------+-----------+-----------+\n",
      "|person_id|gender|T2D_ONLY|AD_ONLY|DEM_ONLY|BOTH_T2D_AD|BOTH_AD_DEM|\n",
      "+---------+------+--------+-------+--------+-----------+-----------+\n",
      "|      148|     F|    null|   null|    null|       null|       null|\n",
      "|      463|     F|    null|   null|    null|       null|       null|\n",
      "|      471|     F|    null|   null|    null|       null|       null|\n",
      "|      833|     M|    null|   null|    null|       null|       null|\n",
      "|     1088|     M|    null|   null|    null|       null|       null|\n",
      "|     1238|     F|    null|   null|    null|       null|       null|\n",
      "|     1342|     F|    null|   null|    null|       null|       null|\n",
      "|     1580|     F|    null|   null|    null|       null|       null|\n",
      "|     1591|     M|    null|   null|    null|       null|       null|\n",
      "|     1645|     F|    null|   null|    null|       null|       null|\n",
      "|     1829|     M|  53.854|   null|    null|       null|       null|\n",
      "|     1959|     F|    null|   null|    null|       null|       null|\n",
      "|     2122|     M|    null|   null|    null|       null|       null|\n",
      "|     2142|     M|    null|   null|    null|       null|       null|\n",
      "|     2366|     F|    null|   null|    null|       null|       null|\n",
      "|     2659|     M|    null|   null|    null|       null|       null|\n",
      "|     3175|     F|    null|   null|    null|       null|       null|\n",
      "|     3794|     F|    null|   null|    null|       null|       null|\n",
      "|     3918|     M|    null|   null|    null|       null|       null|\n",
      "|     3997|     M|    null|   null|    null|       null|       null|\n",
      "+---------+------+--------+-------+--------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+-------+--------+-----------+-----------+\n",
      "|person_id|gender|T2D_ONLY|AD_ONLY|DEM_ONLY|BOTH_T2D_AD|BOTH_AD_DEM|\n",
      "+---------+------+--------+-------+--------+-----------+-----------+\n",
      "|        0|     0|   38695|  49915|   49969|      50232|      49458|\n",
      "+---------+------+--------+-------+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_final.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_final.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of mean in selected null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, exclude=set()): \n",
    "    stats = df.agg(*(\n",
    "     avg(c).alias(c) for c in df.columns if c not in exclude \n",
    "    )) \n",
    "    return df.na.fill(stats.first().asDict()) \n",
    "\n",
    "df_final=fill_with_mean(df_final, [\"person_id\", \"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------------+-----------------+--------------------+------------------+-------------------+\n",
      "|person_id|gender|         T2D_ONLY|          AD_ONLY|            DEM_ONLY|       BOTH_T2D_AD|        BOTH_AD_DEM|\n",
      "+---------+------+-----------------+-----------------+--------------------+------------------+-------------------+\n",
      "|      148|     F|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|      463|     F|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|      471|     F|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|      833|     M|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1088|     M|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "+---------+------+-----------------+-----------------+--------------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|21733|\n",
      "|     M|28637|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.groupby([\"gender\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------------+\n",
      "|person_id|gender|         T2D_ONLY|\n",
      "+---------+------+-----------------+\n",
      "|      148|     F|66.59624051391883|\n",
      "|      463|     F|66.59624051391883|\n",
      "|      471|     F|66.59624051391883|\n",
      "|      833|     M|66.59624051391883|\n",
      "|     1088|     M|66.59624051391883|\n",
      "+---------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.select('person_id','gender','T2D_ONLY').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+\n",
      "|T2D_ONLY_gender|  F|  M|\n",
      "+---------------+---+---+\n",
      "|         64.013|  1|  0|\n",
      "|         63.479|  0|  1|\n",
      "|         54.665|  0|  1|\n",
      "|         74.361|  0|  1|\n",
      "|         77.988|  0|  2|\n",
      "|         84.822|  0|  1|\n",
      "|         58.253|  3|  0|\n",
      "|         76.129|  0|  1|\n",
      "|         71.094|  3|  0|\n",
      "|         64.898|  0|  1|\n",
      "|         59.402|  0|  1|\n",
      "|          67.86|  0|  1|\n",
      "|          52.18|  4|  0|\n",
      "|         55.432|  0|  1|\n",
      "|         40.366|  1|  0|\n",
      "|         70.671|  1|  0|\n",
      "|         78.558|  1|  0|\n",
      "|         78.211|  3|  0|\n",
      "|         61.638|  0|  1|\n",
      "|         52.156|  0|  1|\n",
      "+---------------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.crosstab('T2D_ONLY', 'gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     F|\n",
      "|     F|\n",
      "|     M|\n",
      "|     M|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.select('gender').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_final=df_final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_id      0\n",
       "gender         0\n",
       "T2D_ONLY       0\n",
       "AD_ONLY        0\n",
       "DEM_ONLY       0\n",
       "BOTH_T2D_AD    0\n",
       "BOTH_AD_DEM    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "pd_final.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    28637\n",
       "F    21733\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting gender \n",
    "pd_final['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(pd_final, kind='reg', plot_kws={'line_kws':{'color': 'cyan'}})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final1=df_final.drop('person_id') # PID not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[person_id: int, gender: string, T2D_ONLY: double, AD_ONLY: double, DEM_ONLY: double, BOTH_T2D_AD: double, BOTH_AD_DEM: double]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final1=df_final.select('person_id','T2D_ONLY','AD_ONLY','DEM_ONLY','BOTH_T2D_AD','BOTH_AD_DEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------------+--------------------+------------------+-------------------+\n",
      "|person_id|         T2D_ONLY|          AD_ONLY|            DEM_ONLY|       BOTH_T2D_AD|        BOTH_AD_DEM|\n",
      "+---------+-----------------+-----------------+--------------------+------------------+-------------------+\n",
      "|      148|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|      463|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|      471|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|      833|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1088|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1238|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1342|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1580|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1591|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1645|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1829|           53.854|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     1959|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     2122|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     2142|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     2366|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     2659|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     3175|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     3794|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     3918|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "|     3997|66.59624051391883|80.04218681318676|0.008650872817955116|-0.773536231884058|-1.0035460526315794|\n",
      "+---------+-----------------+-----------------+--------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pyspark.sql import Row\n",
    "def flatten_table(column_names, column_values):\n",
    "    row = zip(column_names, column_values)\n",
    "    _, person_id = next(row)  # Special casing retrieving the first column\n",
    "    return [\n",
    "        Row(person_id=person_id,Disease_group=column, Disease_Age=value)\n",
    "        for column, value in row\n",
    "    ]\n",
    "    \n",
    "df_final1=df_final1.rdd.flatMap(partial(flatten_table, df_final1.columns)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+\n",
      "|person_id|Disease_group|         Disease_Age|\n",
      "+---------+-------------+--------------------+\n",
      "|      148|     T2D_ONLY|   66.59624051391883|\n",
      "|      148|      AD_ONLY|   80.04218681318676|\n",
      "|      148|     DEM_ONLY|0.008650872817955116|\n",
      "|      148|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      148|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "|      463|     T2D_ONLY|   66.59624051391883|\n",
      "|      463|      AD_ONLY|   80.04218681318676|\n",
      "|      463|     DEM_ONLY|0.008650872817955116|\n",
      "|      463|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      463|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "|      471|     T2D_ONLY|   66.59624051391883|\n",
      "|      471|      AD_ONLY|   80.04218681318676|\n",
      "|      471|     DEM_ONLY|0.008650872817955116|\n",
      "|      471|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      471|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "|      833|     T2D_ONLY|   66.59624051391883|\n",
      "|      833|      AD_ONLY|   80.04218681318676|\n",
      "|      833|     DEM_ONLY|0.008650872817955116|\n",
      "|      833|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      833|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "+---------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2=df_final.select('person_id','gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_final2.join(df_final1.select('person_id', 'Disease_group','Disease_Age'), ['person_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------+--------------------+\n",
      "|person_id|gender|Disease_group|         Disease_Age|\n",
      "+---------+------+-------------+--------------------+\n",
      "|      148|     F|     T2D_ONLY|   66.59624051391883|\n",
      "|      148|     F|      AD_ONLY|   80.04218681318676|\n",
      "|      148|     F|     DEM_ONLY|0.008650872817955116|\n",
      "|      148|     F|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      148|     F|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "|      463|     F|     T2D_ONLY|   66.59624051391883|\n",
      "|      463|     F|      AD_ONLY|   80.04218681318676|\n",
      "|      463|     F|     DEM_ONLY|0.008650872817955116|\n",
      "|      463|     F|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      463|     F|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "|      471|     F|     T2D_ONLY|   66.59624051391883|\n",
      "|      471|     F|      AD_ONLY|   80.04218681318676|\n",
      "|      471|     F|     DEM_ONLY|0.008650872817955116|\n",
      "|      471|     F|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      471|     F|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "|      833|     M|     T2D_ONLY|   66.59624051391883|\n",
      "|      833|     M|      AD_ONLY|   80.04218681318676|\n",
      "|      833|     M|     DEM_ONLY|0.008650872817955116|\n",
      "|      833|     M|  BOTH_T2D_AD|  -0.773536231884058|\n",
      "|      833|     M|  BOTH_AD_DEM| -1.0035460526315794|\n",
      "+---------+------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = ['person_id','gender']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'Disease_group', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['Disease_Age']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Disease_group: string (nullable = true)\n",
      " |-- Disease_Age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "cols = df_final.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df_final)\n",
    "df_final = pipelineModel.transform(df_final)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df_final =df_final.select(selectedCols)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING USING SPARK ML ON THE PIPELINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 232043\n",
      "Test Dataset Count: 99467\n"
     ]
    }
   ],
   "source": [
    "train, test = df_final.randomSplit([0.7, 0.3], seed = 4000)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION CLASSIFIER\n",
    "\n",
    "Parameters : maxIter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions on test data using the transform() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "|  0.0|       0.0|[0.99932778726371...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"label\",\"prediction\", \"probability\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC curve:  0.8774112038071622\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC curve: ', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|       19930|\n",
      "|  1.0|       19906|\n",
      "|  4.0|       19711|\n",
      "|  3.0|       20034|\n",
      "|  2.0|       19886|\n",
      "+-----+------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|            22180|\n",
      "|       1.0|             6995|\n",
      "|       4.0|            18365|\n",
      "|       3.0|            17291|\n",
      "|       2.0|            34636|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")\n",
    "cm.groupby('label').agg({'label': 'count'}).show()\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 23.418%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = lrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC for k=5 CV : 0.9907905506631922\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "predictions2 = cvModel.transform(test)\n",
    "print('Test Area Under ROC for k=5 CV :', evaluator.evaluate(predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for logistic regression model on test data with k=5 CV: 20.050%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy for logistic regression model on test data with k=5 CV: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('label', 'prediction', 'probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC curve for DT classifier : ', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for Decision tree classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")\n",
    "cm.groupby('label').agg({'label': 'count'}).show()\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy for DT classifier: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = dtModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE OF DECISION TREES : RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('label','prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC curve for RF classifier : ', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for random forest classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")\n",
    "cm.groupby('label').agg({'label': 'count'}).show()\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy for RF classifier: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = rfModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FEATURE IMPORTANCE FROM RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ExtractFeatureImp(rfModel.featureImportances, df_final1, \"features\").head(10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_final1\n",
    "df2 = rfModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "varlist = ExtractFeatureImp(rfModel.featureImportances,df, \"features\")\n",
    "varidx = [x for x in varlist['idx'][0:10]]\n",
    "varidx\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidx)\n",
    "\n",
    "df3 = slicer.transform(df2)\n",
    "\n",
    "df3 = df3.drop('rawPrediction', 'probability', 'prediction')\n",
    "rf2 = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features2\", seed = 8464,\n",
    "                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "mod2 = rf2.fit(df3)\n",
    "pred = mod2.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOST TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select('label','prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC curve for GBT classifier : ', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for GRADIENT BOOST TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")\n",
    "cm.groupby('label').agg({'label': 'count'}).show()\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy for RF classifier: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = rfModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE IMPORTANCE FROM THE TREE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel.featureImportances # Add this for report\n",
    "dtModel.featureImportances # Add this for report\n",
    "gbtModel.featureImportances # Add this for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = dtModel.featureImportances\n",
    "\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "result1 = ExtractFeatureImp(dtModel.featureImportances,df, \"features\").head(4)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = gbtModel.featureImportances\n",
    "\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "result2 = ExtractFeatureImp(gbtModel.featureImportances,df, \"features\").head(10)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = rfModel.featureImportances\n",
    "\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "result3 = ExtractFeatureImp(rfModel.featureImportances,df, \"features\").head(10)\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([result1, result2, result3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values(by='score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving MIMIC  as mimic_final CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final1=pd.read_csv(\"final_mimic.csv\",index_col=0) #this is saving pyspark dataframe to a new csv file named final_mimic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
